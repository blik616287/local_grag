version: '3.8'

services:
  # Graph Database
  neo4j:
    image: neo4j:5.15-community
    container_name: graphrag-neo4j
    ports:
      - "7474:7474"  # Browser UI
      - "7687:7687"  # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/graphrag123
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2g
      - NEO4J_dbms_memory_pagecache_size=1g
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p graphrag123 'RETURN 1'"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - graphrag-network

  # PostgreSQL for state management + optional vector store
  postgres:
    image: pgvector/pgvector:pg16
    container_name: graphrag-postgres
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_DB=graphrag
      - POSTGRES_USER=graphrag
      - POSTGRES_PASSWORD=graphrag123
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U graphrag"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - graphrag-network

  # Redis for caching (keeping for other use cases)
  redis:
    image: redis:7-alpine
    container_name: graphrag-redis
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - graphrag-network

  # RabbitMQ for message queuing
  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: graphrag-rabbitmq
    ports:
      - "5672:5672"   # AMQP protocol
      - "15672:15672" # Management UI
    environment:
      - RABBITMQ_DEFAULT_USER=graphrag
      - RABBITMQ_DEFAULT_PASS=graphrag123
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - graphrag-network

  # GPU Inference Server (vLLM)
  vllm:
    image: vllm/vllm-openai:latest
    container_name: graphrag-vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HF_HOME=/models
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
    command: >
      --model Qwen/Qwen2.5-1.5B-Instruct
      --host 0.0.0.0
      --port 8000
      --download-dir /models
      --gpu-memory-utilization 0.5
      --max-model-len 2048
      --dtype half
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Model loading takes time
    networks:
      - graphrag-network

  # Embedding Service (Sentence Transformers)
  embeddings:
    build:
      context: ./services/embeddings
      dockerfile: Dockerfile
    container_name: graphrag-embeddings
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
      - DEVICE=cuda
      - BATCH_SIZE=32
      - HF_HOME=/models
    ports:
      - "8001:8001"
    volumes:
      - ./models:/models
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s
    networks:
      - graphrag-network

  # Ingestion Service
  ingestion:
    build:
      context: ./services/ingestion
      dockerfile: Dockerfile
    container_name: graphrag-ingestion
    depends_on:
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vllm:
        condition: service_healthy
      embeddings:
        condition: service_healthy
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=graphrag123
      - POSTGRES_URI=postgresql://graphrag:graphrag123@postgres:5432/graphrag
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - VLLM_ENDPOINT=http://vllm:8000/v1
      - EMBEDDINGS_ENDPOINT=http://embeddings:8001
      - WORKER_CONCURRENCY=4
      - EXTRACTION_DIR=/tmp/extraction
    volumes:
      - ./services/ingestion:/app
      - ingestion_cache:/tmp/extraction
    command: python worker.py
    networks:
      - graphrag-network
    restart: unless-stopped

  # Query API Service
  api:
    build:
      context: ./services/query-api
      dockerfile: Dockerfile
    container_name: graphrag-api
    depends_on:
      neo4j:
        condition: service_healthy
      embeddings:
        condition: service_started
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=graphrag123
      - VLLM_ENDPOINT=http://vllm:8000/v1
      - EMBEDDINGS_ENDPOINT=http://embeddings:8001
      - LLM_MODEL=microsoft/Phi-3-mini-4k-instruct
    ports:
      - "8002:8002"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - graphrag-network
    restart: unless-stopped

  # Backend API for UI
  backend:
    build:
      context: ./services/backend
      dockerfile: Dockerfile
    container_name: graphrag-backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - POSTGRES_URI=postgresql://graphrag:graphrag123@postgres:5432/graphrag
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - VLLM_ENDPOINT=http://vllm:8000/v1
    ports:
      - "5000:5000"
    volumes:
      - ./services/backend:/app
    command: python app.py
    networks:
      - graphrag-network
    restart: unless-stopped

  # Web UI (React)
  frontend:
    build:
      context: ./services/frontend
      dockerfile: Dockerfile
    container_name: graphrag-frontend
    depends_on:
      - backend
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=http://localhost:5000
    volumes:
      - ./services/frontend:/app
      - /app/node_modules
    networks:
      - graphrag-network

  # Validation Service
  validation:
    build:
      context: ./services/validation
      dockerfile: Dockerfile
    container_name: graphrag-validation
    depends_on:
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=graphrag123
      - POSTGRES_URI=postgresql://graphrag:graphrag123@postgres:5432/graphrag
    ports:
      - "8003:8003"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8003/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - graphrag-network
    restart: unless-stopped

volumes:
  neo4j_data:
  neo4j_logs:
  neo4j_import:
  neo4j_plugins:
  postgres_data:
  redis_data:
  rabbitmq_data:
  ingestion_cache:

networks:
  graphrag-network:
    name: graphrag-network
    driver: bridge
